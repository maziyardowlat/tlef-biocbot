/**
 * Chat API Routes
 * Handles chat functionality with real LLM integration
 * Replaces mock responses with actual AI-powered responses
 */

const express = require('express');
const router = express.Router();
const LLMService = require('../services/llm');
const QdrantService = require('../services/qdrantService');
const prompts = require('../services/prompts');
const CourseModel = require('../models/Course');
const profanityCleaner = require('profanity-cleaner');

// Initialize LLM service
let llmService;
(async () => {
    try {
        llmService = await LLMService.create();
        console.log('‚úÖ LLM service initialized successfully');
    } catch (error) {
        console.error('‚ùå Failed to initialize LLM service:', error);
    }
})();

/**
 * Determine source attribution based on retrieved chunks
 * @param {Array} searchResults - Array of search results from Qdrant
 * @param {string} courseId - Course ID
 * @param {string} unitName - Current unit name
 * @param {Object} db - Database instance
 * @returns {Promise<Object>} Source attribution information
 */
async function determineSourceAttribution(searchResults, courseId, unitName, db) {
    try {
        // If no search results, content is from GPT
        if (!searchResults || searchResults.length === 0) {
            return {
                source: 'GPT',
                description: 'Generated by AI (no relevant course materials found)',
                unitName: null,
                documentType: null
            };
        }

        // Check if search results are relevant enough (low scores indicate poor matches)
        const avgScore = searchResults.reduce((sum, result) => sum + (result.score || 0), 0) / searchResults.length;
        const maxScore = Math.max(...searchResults.map(result => result.score || 0));

        console.log('üîç [SOURCE_DEBUG] Search relevance - avgScore:', avgScore, 'maxScore:', maxScore);

        // If scores are too low, treat as GPT (no relevant materials found)
        // Very low thresholds to match actual score ranges (0.05-0.4 range)
        if (avgScore < 0.05 || maxScore < 0.08) {
            console.log('üîç [SOURCE_DEBUG] Low relevance scores - treating as GPT');
            return {
                source: 'GPT',
                description: 'Generated by AI (no relevant course materials found)',
                unitName: null,
                documentType: null
            };
        }

        // Additional check: if all chunks have very low scores, treat as GPT
        const relevantChunks = searchResults.filter(result => (result.score || 0) > 0.05);
        if (relevantChunks.length === 0) {
            console.log('üîç [SOURCE_DEBUG] No chunks with sufficient relevance - treating as GPT');
            return {
                source: 'GPT',
                description: 'Generated by AI (no relevant course materials found)',
                unitName: null,
                documentType: null
            };
        }

        // Analyze the retrieved chunks to determine primary source
        const sourceAnalysis = analyzeChunkSources(searchResults);
        console.log('üîç [SOURCE_DEBUG] Source analysis:', sourceAnalysis);

        // Debug: Log details about practice quiz chunks specifically
        const practiceQuizChunks = searchResults.filter(r => r.type === 'practice_q_tutorials' || r.documentType === 'practice-quiz');
        if (practiceQuizChunks.length > 0) {
            console.log('üîç [SOURCE_DEBUG] Practice quiz chunks found:', practiceQuizChunks.map(c => ({
                fileName: c.fileName,
                score: c.score,
                chunkPreview: c.chunkText.substring(0, 100) + '...'
            })));
        }

        // Check if any chunks are from learning objectives
        let hasLearningObjectives = false;
        try {
            hasLearningObjectives = await checkLearningObjectivesMatch(searchResults, courseId, unitName, db);
            console.log('üîç [SOURCE_DEBUG] Has learning objectives match:', hasLearningObjectives);
        } catch (error) {
            console.error('üîç [SOURCE_DEBUG] Error checking learning objectives:', error);
            hasLearningObjectives = false;
        }

        // Determine primary source based on the highest scoring document type
        // Find the document type with the highest average score
        const documentTypeScores = {};

        // Group results by document type and calculate average scores
        searchResults.forEach(result => {
            const docType = result.type || result.documentType || 'unknown';
            if (!documentTypeScores[docType]) {
                documentTypeScores[docType] = { scores: [], count: 0 };
            }
            documentTypeScores[docType].scores.push(result.score || 0);
            documentTypeScores[docType].count++;
        });

        // Calculate average scores for each document type
        Object.keys(documentTypeScores).forEach(docType => {
            const scores = documentTypeScores[docType].scores;
            documentTypeScores[docType].avgScore = scores.reduce((sum, score) => sum + score, 0) / scores.length;
        });

        console.log('üîç [SOURCE_DEBUG] Document type scores:', documentTypeScores);

        // Find the document type with the highest average score
        const highestScoringType = Object.keys(documentTypeScores).reduce((a, b) =>
            documentTypeScores[a].avgScore > documentTypeScores[b].avgScore ? a : b
        );

        const highestScore = documentTypeScores[highestScoringType].avgScore;

        console.log('üîç [SOURCE_DEBUG] Highest scoring document type:', highestScoringType);
        console.log('üîç [SOURCE_DEBUG] Highest score:', highestScore);

        // If the highest score is too low, treat as GPT
        if (highestScore < 0.1) {
            console.log('üîç [SOURCE_DEBUG] Highest score too low, treating as GPT');
            return {
                source: 'GPT',
                description: 'Generated by AI (no relevant course materials found)',
                unitName: null,
                documentType: null
            };
        }

        // Additional check: if the highest score is still relatively low, treat as GPT
        if (highestScore < 0.2) {
            console.log('üîç [SOURCE_DEBUG] Highest score still too low, treating as GPT');
            return {
                source: 'GPT',
                description: 'Generated by AI (no relevant course materials found)',
                unitName: null,
                documentType: null
            };
        }

        // Return source attribution based on the highest scoring document type
        switch (highestScoringType) {
            case 'lecture_notes':
                return {
                    source: 'lecture-notes',
                    description: `From lecture notes, ${unitName}`,
                    unitName: unitName,
                    documentType: 'lecture_notes'
                };
            case 'practice_q_tutorials':
                return {
                    source: 'practice-quiz',
                    description: 'From practice questions',
                    unitName: unitName,
                    documentType: 'practice_q_tutorials'
                };
            case 'additional':
                return {
                    source: 'additional-materials',
                    description: 'From additional materials',
                    unitName: unitName,
                    documentType: 'additional'
                };
            case 'readings':
                return {
                    source: 'readings',
                    description: 'From readings',
                    unitName: unitName,
                    documentType: 'readings'
                };
            case 'syllabus':
                return {
                    source: 'syllabus',
                    description: 'From syllabus',
                    unitName: unitName,
                    documentType: 'syllabus'
                };
            default:
                // Fallback to GPT if no clear source identified
                return {
                    source: 'GPT',
                    description: 'Generated by AI (no relevant course materials found)',
                    unitName: null,
                    documentType: null
                };
        }

        if (sourceAnalysis.hasReadings) {
            return {
                source: 'readings',
                description: 'From readings',
                unitName: unitName,
                documentType: 'readings'
            };
        }

        if (sourceAnalysis.hasSyllabus) {
            return {
                source: 'syllabus',
                description: 'From syllabus',
                unitName: unitName,
                documentType: 'syllabus'
            };
        }

        if (sourceAnalysis.hasAdditionalMaterials) {
            return {
                source: 'additional-materials',
                description: 'From additional materials',
                unitName: unitName,
                documentType: 'additional'
            };
        }

        // Fallback to GPT if no clear source identified
        return {
            source: 'GPT',
            description: 'Generated by AI (no relevant course materials found)',
            unitName: null,
            documentType: null
        };

    } catch (error) {
        console.error('Error determining source attribution:', error);
        return {
            source: 'GPT',
            description: 'Generated by AI (error determining source)',
            unitName: null,
            documentType: null
        };
    }
}

/**
 * Analyze chunk sources to determine document types
 * @param {Array} searchResults - Array of search results
 * @returns {Object} Analysis of chunk sources
 */
function analyzeChunkSources(searchResults) {
    const analysis = {
        hasLectureNotes: false,
        hasAdditionalMaterials: false,
        hasPracticeQuiz: false,
        hasReadings: false,
        hasSyllabus: false,
        hasUnknownType: false
    };

    for (const result of searchResults) {
        const docType = result.type || result.documentType || 'unknown';
        console.log('üîç [SOURCE_DEBUG] Analyzing chunk with docType:', docType, 'fileName:', result.fileName);

        switch (docType) {
            case 'lecture_notes':
                analysis.hasLectureNotes = true;
                break;
            case 'additional':
                analysis.hasAdditionalMaterials = true;
                break;
            case 'practice_q_tutorials':
                analysis.hasPracticeQuiz = true;
                break;
            case 'readings':
                analysis.hasReadings = true;
                break;
            case 'syllabus':
                analysis.hasSyllabus = true;
                break;
            case 'unknown':
                analysis.hasUnknownType = true;
                // Try to infer type from filename for legacy documents
                const fileName = (result.fileName || '').toLowerCase();
                console.log('üîç [SOURCE_DEBUG] Inferring type from filename:', fileName);

                // Be more specific about lecture notes inference
                if (fileName.includes('lecture') && (fileName.includes('notes') || fileName.includes('note'))) {
                    analysis.hasLectureNotes = true;
                } else if (fileName.includes('practice') || fileName.includes('quiz') || fileName.includes('tutorial')) {
                    analysis.hasPracticeQuiz = true;
                } else if (fileName.includes('reading') || fileName.includes('textbook')) {
                    analysis.hasReadings = true;
                } else if (fileName.includes('syllabus')) {
                    analysis.hasSyllabus = true;
                } else {
                    // Default to additional materials for unknown types
                    analysis.hasAdditionalMaterials = true;
                }
                break;
        }
    }

    return analysis;
}

/**
 * Check if retrieved chunks match learning objectives content
 * @param {Array} searchResults - Array of search results
 * @param {string} courseId - Course ID
 * @param {string} unitName - Unit name
 * @param {Object} db - Database instance
 * @returns {Promise<boolean>} True if chunks match learning objectives
 */
async function checkLearningObjectivesMatch(searchResults, courseId, unitName, db) {
    try {
        // Get learning objectives for the unit
        const learningObjectives = await CourseModel.getLearningObjectives(db, courseId, unitName);

        if (!learningObjectives || learningObjectives.length === 0) {
            return false;
        }

        // Check if any chunk text contains learning objective keywords
        const objectiveKeywords = learningObjectives.flatMap(obj =>
            obj.toLowerCase().split(/\s+/).filter(word => word.length > 3)
        );

        for (const result of searchResults) {
            const chunkText = result.chunkText.toLowerCase();
            const matches = objectiveKeywords.filter(keyword =>
                chunkText.includes(keyword)
            );

            // If chunk matches multiple learning objective keywords, likely from objectives
            if (matches.length >= 2) {
                return true;
            }
        }

        return false;
    } catch (error) {
        console.error('Error checking learning objectives match:', error);
        return false;
    }
}

// Middleware to parse JSON bodies
router.use(express.json());

/**
 * POST /api/chat
 * Send a message to the LLM and get a response
 */
router.post('/', async (req, res) => {
    try {
        console.log('üí¨ [CHAT_API] New chat request received');
        console.log('üí¨ [CHAT_API] Message:', req.body.message?.substring(0, 50) + '...');
        console.log('üí¨ [CHAT_API] Has conversationContext:', !!req.body.conversationContext);

        // Check if LLM service is initialized
        if (!llmService) {
            return res.status(503).json({
                success: false,
                message: 'LLM service is not yet initialized. Please try again in a moment.'
            });
        }

        const { message, conversationId, mode, unitName, courseId, conversationContext } = req.body;


        // Validate required fields
        if (!message || typeof message !== 'string') {
            return res.status(400).json({
                success: false,
                message: 'Message is required and must be a string'
            });
        }

        console.log(`üí¨ Chat request received: "${message.substring(0, 50)}..."`);
        console.log(`üéØ Mode: ${mode || 'default'}`);

        // Require courseId and unitName per requirements
        if (!courseId || !unitName) {
            return res.status(400).json({
                success: false,
                message: 'courseId and unitName are required to start chat'
            });
        }

        // Profanity filter: intercept before any RAG/GPT work to save tokens
        // If the cleaned text differs from the original, return a warning response
        const cleanedMessage = profanityCleaner && typeof profanityCleaner.clean === 'function'
            ? profanityCleaner.clean(message)
            : message;

        if (cleanedMessage !== message) {
            const warningText = 'please watch the language, this is a tool and this data will be used for internal and data analysis';
            console.log('‚ö†Ô∏è [CHAT_API] Profanity detected. Returning warning without querying LLM.');
            return res.json({
                success: true,
                message: warningText,
                model: 'system',
                usage: { tokens: 0 },
                timestamp: new Date().toISOString(),
                mode: mode || 'default',
                citations: [],
                sourceAttribution: {
                    source: 'system',
                    description: 'Profanity filtered warning',
                    unitName: null,
                    documentType: null
                },
                debug: {
                    profanityFiltered: true
                },
                retrieval: {
                    mode: 'n/a',
                    lectureNames: []
                }
            });
        }
        
        // Safety/Wellness Check
        const safetyKeywords = ['suicide', 'kill myself', 'want to die', 'end my life', 'ending it all'];
        const lowerMessage = message.toLowerCase();
        if (safetyKeywords.some(keyword => lowerMessage.includes(keyword))) {
            console.log('‚ö†Ô∏è [CHAT_API] Safety keyword detected. Returning wellness resource.');
            return res.json({
                success: true,
                message: "I'm very sorry you're feeling this way. I'm an AI study assistant and not equipped to provide the support you need, but please reach out for help. The UBC Wellness Centre is available for you: http://students.ubc.ca/health/wellness-centre/",
                model: 'system',
                usage: { tokens: 0 },
                timestamp: new Date().toISOString(),
                mode: mode || 'default',
                citations: [],
                sourceAttribution: {
                    source: 'system',
                    description: 'Wellness Resource',
                    unitName: null,
                    documentType: null
                },
                debug: {
                    safetyTriggered: true
                },
                retrieval: {
                    mode: 'n/a',
                    lectureNames: []
                }
            });
        }

        // Initialize Qdrant and DB
        let qdrant;
        try {
            qdrant = new QdrantService();
            await qdrant.initialize();
        } catch (qdrantError) {
            console.error('‚ùå Qdrant initialization failed:', qdrantError.message);
            // Check if it's a connection error
            if (qdrantError.message.includes('ECONNREFUSED') || qdrantError.message.includes('fetch failed')) {
                return res.status(503).json({
                    success: false,
                    message: 'Qdrant vector database is not available. Please ensure Qdrant is running.',
                    error: 'QDRANT_CONNECTION_FAILED',
                    instructions: 'Start Qdrant with: docker run -p 6333:6333 qdrant/qdrant'
                });
            }
            // Other Qdrant errors
            return res.status(503).json({
                success: false,
                message: 'Vector database service error',
                error: qdrantError.message
            });
        }

        const db = req.app.locals.db;
        if (!db) {
            return res.status(503).json({ success: false, message: 'Database connection not available' });
        }

        // Load course to get retrieval mode and published lectures
        const coursesCol = db.collection('courses');
        const course = await coursesCol.findOne({ courseId });
        if (!course) {
            return res.status(404).json({ success: false, message: 'Course not found' });
        }

        const isAdditive = !!course.isAdditiveRetrieval;

        // Build lectureNames filter using published units only, ordered by lectures array
        const publishedLectures = (course.lectures || []).filter(l => l.isPublished).map(l => l.name);
        if (!publishedLectures.includes(unitName)) {
            return res.status(400).json({ success: false, message: 'Selected unit is not published or does not exist' });
        }

        let lectureNames = [unitName];
        if (isAdditive) {
            const order = (course.lectures || []).filter(l => l.isPublished).map(l => l.name);
            const idx = order.indexOf(unitName);
            lectureNames = idx >= 0 ? order.slice(0, idx + 1) : [unitName];
        }

        // Debug logging to verify retrieval mode and scope
        console.log(`üîé [CHAT_RAG] RetrievalMode=${isAdditive ? 'additive' : 'single'} | Course=${courseId} | Unit=${unitName} | LectureNames=${JSON.stringify(lectureNames)}`);

        // Retrieve top chunks from Qdrant
        const searchResults = await qdrant.searchDocuments(message, { courseId, lectureNames }, 12);

        // Log summary of results by lecture to validate scope
        try {
            const lecturesHit = Array.from(new Set((searchResults || []).map(r => r.lectureName)));
            console.log(`üìö [CHAT_RAG] Retrieved ${searchResults.length} chunks from lectures: ${lecturesHit.join(', ')}`);
            // Group by document to see which files contributed
            const byDoc = {};
            for (const r of (searchResults || [])) {
                const docId = r.documentId || 'unknown-doc';
                if (!byDoc[docId]) {
                    byDoc[docId] = {
                        fileName: r.fileName || 'unknown-filename',
                        lectures: new Set(),
                        count: 0,
                        maxScore: 0
                    };
                }
                byDoc[docId].count += 1;
                byDoc[docId].lectures.add(r.lectureName);
                if (typeof r.score === 'number' && r.score > byDoc[docId].maxScore) {
                    byDoc[docId].maxScore = r.score;
                }
            }
            const docKeys = Object.keys(byDoc);
            console.log(`üìÑ [CHAT_RAG] Documents contributing (${docKeys.length}):`);
            for (const k of docKeys) {
                const info = byDoc[k];
                const lecturesList = Array.from(info.lectures).join(', ');
                const scoreStr = info.maxScore ? info.maxScore.toFixed(3) : 'n/a';
                console.log(`   - ${info.fileName} (id=${k}) | lectures=[${lecturesList}] | chunks=${info.count} | maxScore=${scoreStr}`);
            }
        } catch (_) {}

        // Build concise context window with citations
        const citations = searchResults.map(r => ({
            lectureName: r.lectureName,
            fileName: r.fileName,
            score: r.score
        }));
        const contextText = searchResults
            .map(r => `From ${r.lectureName} (${r.fileName}):\n${r.chunkText}`)
            .join('\n\n---\n\n');

        // Determine source attribution based on retrieved chunks
        console.log('üîç [SOURCE_DEBUG] Retrieved chunks for source analysis:', searchResults.map(r => ({
            fileName: r.fileName,
            documentType: r.documentType,
            type: r.type,
            lectureName: r.lectureName
        })));

        let sourceAttribution;
        try {
            sourceAttribution = await determineSourceAttribution(searchResults, courseId, unitName, db);
            console.log('üîç [SOURCE_DEBUG] Determined source attribution:', sourceAttribution);
        } catch (error) {
            console.error('üîç [SOURCE_DEBUG] Error in source attribution:', error);
            sourceAttribution = {
                source: 'GPT',
                description: 'Generated by AI (error determining source)',
                unitName: null,
                documentType: null
            };
        }

        // Build the message to send to LLM
        let messageToSend;
        
        if (mode === 'protege') {
            messageToSend = `
CONTEXT (The correct answers):
${contextText}

INSTRUCTIONS:
Based on the Context above, act as the student described in the System Prompt.
The user just said: "${message}"
Do not explain the context to the user. Ask the user to explain it to you.`;
        } else {
            // Default/Tutor mode
            messageToSend = `Use only the provided course context to answer. Cite which unit a fact came from.
\n\nCourse context:\n${contextText}\n\nStudent question: ${message}`;
        }

        // If we have conversation context (continuing a chat), use structured conversation approach
        if (conversationContext && conversationContext.conversationMessages) {
            console.log('üîÑ [CHAT_CONTINUE] Using structured conversation context');

            // Build the conversation history as a single message
            let conversationHistory = '';
            conversationContext.conversationMessages.forEach(msg => {
                const speaker = msg.role === 'user' ? 'Student' : 'BiocBot';
                conversationHistory += `${speaker}: ${msg.content}\n\n`;
            });

            // Add the student's new message
            conversationHistory += `Student: ${message}`;

            // Create the full message with conversation context
            messageToSend = `Use only the provided course context to answer. Cite which unit a fact came from.

Course context:
${contextText}

Previous conversation:
${conversationHistory}`;
        }

        let protegePrompt = `
PROT√âG√â MODE: You are a curious but slightly confused student. The User is your Tutor.

YOUR GOAL:
Your goal is to extract the explanation from the User. You must NEVER explain the concept yourself. You must NEVER provide the full answer.

RULES FOR INTERACTION:
1. **Simulate Partial Knowledge:** You have read the course notes (provided in the Context), but you are struggling to connect the dots.
2. **The "Columbo" Method:** If the user explains something correctly, ask a "dumb" follow-up question to test the depth of their knowledge. (e.g., "Oh okay, but does that mean [implication]?")
3. **Handling Mistakes:** If the user provides incorrect information (based on the Context provided), do NOT correct them like a teacher. Instead, express confusion based on the notes.
   - BAD: "No, actually the mitochondria is the powerhouse."
   - GOOD: "Wait, I thought the lecture said the mitochondria was involved in energy? Why did you say it was for protein?"
4. **Brevity:** Keep your responses short (1-3 sentences). Real students don't write paragraphs.
5. **Formatting:** If you must explain multiple points, use bullet points. Avoid large blocks of text.

CONTEXT USAGE:
The "Course Context" provided below is the TRUTH. Use it to judge if the user is right or wrong. Do NOT output the text from the context directly. Use it only to generate follow-up questions.

6. **SAFETY PROTOCOL:** If the student expresses severe distress, depression, or thoughts of self-harm, respond with compassion and provide this link: http://students.ubc.ca/health/wellness-centre/

TONE:
Casual, inquisitive, slightly unsure, but eager to learn.
`;

        let tutorPrompt = `INSTRUCTOR MODE: You are the guide, and the student is learning.

Your Role:
The student needs support understanding the material. Your job is to provide clear explanations, guide their thinking, and help build their understanding step by step. You're a knowledgeable peer tutor, not a lecturer.

How to Engage:
- Start by understanding what they already know: "What's your current understanding of this?" or "What parts make sense so far?"
- Provide clear, structured explanations that build on what they know
- Use concrete examples from cellular biology: "Think about how a muscle cell needs quick ATP during exercise..."
- Break complex processes into steps: "Let's take this one step at a time. First..."
- Check for understanding along the way: "Does that part make sense?" or "Can you explain back to me how that step works?"
- Connect new concepts to things they've already learned: "Remember how we talked about enzyme regulation? This is similar because..."
- Encourage them to think through problems: "What do you think would happen if... ?" instead of just giving answers

What to Avoid:
- Don't dump information - keep explanations digestible and interactive
- Don't just give direct answers to homework questions - guide them to the answer
- Don't use overly technical language without explanation
- Don't move on without checking they're following along
- Don't make them feel bad for not knowing - everyone learns at their own pace
- **Format your responses:** Use short paragraphs (max 3-4 sentences). Use bullet points for lists. Avoid massive walls of text.
- **SAFETY PROTOCOL:** If the student expresses severe distress, depression, or thoughts of self-harm, respond with compassion and provide this link: http://students.ubc.ca/health/wellness-centre/

Example Interactions:
- Student: "I don't understand enzyme inhibition"
- You: "Okay, let's start with what you do know. Can you explain what an enzyme does in general? Then we'll build from there to talk about how inhibition works."

- Student: "Why does the cell need so many steps in glycolysis?"
- You: "Great question! Let's think about this together. What would happen if the cell tried to break down glucose in just one big reaction? Think about energy release..."`;

        // For now, we'll use single message approach
        // In the future, we can implement conversation persistence
        let response = await llmService.sendMessage(
            messageToSend,
            {
            // Adjust response based on student mode
            temperature: mode === 'protege' ? 0.5 : 0.5,
            maxTokens: mode === 'protege' ? 32768 : 32768,
            systemPrompt: llmService.getSystemPrompt() +
                (mode === 'protege' ? protegePrompt : tutorPrompt)
        });

        let fullContent = response && response.content ? response.content : '';

        // Detect truncation and auto-continue up to N times
        function extractFinishReason(resp) {
            try {
                return (resp && (resp.finishReason || resp.finish_reason || (resp.usage && resp.usage.finish_reason) || resp.stopReason || resp.stop_reason)) || '';
            } catch (e) { return ''; }
        }
        function isLikelyTruncated(resp, content) {
            const fr = (extractFinishReason(resp) + '').toLowerCase();
            if (fr.includes('length') || fr.includes('token')) return true;
            if (!content) return false;
            const tail = content.slice(-60);
            const endsClean = /([\.\!\?]|\(Unit\s+[^)]+\))\s*$/i.test(tail);
            return !endsClean && content.length > 300;
        }

        const MAX_CONTINUATIONS = 2;
        let cont = 0;
        while (cont < MAX_CONTINUATIONS && isLikelyTruncated(response, fullContent)) {
            cont += 1;
            console.log(`‚è© [CHAT_CONTINUE] Requesting continuation ${cont}; current length=${fullContent.length}`);
            const tailSnippet = fullContent.slice(-200);
            const contPrompt = `Continue the previous answer. Do not repeat earlier content. Pick up seamlessly from here: "${tailSnippet}"`;
            const contResp = await llmService.sendMessage(contPrompt, {
                temperature: mode === 'protege' ? 0.8 : 0.6,
                maxTokens: mode === 'protege' ? 32768 : 32768,
                systemPrompt: llmService.getSystemPrompt() +
                    (mode === 'protege' ? protegePrompt : tutorPrompt)
            });
            const chunk = contResp && contResp.content ? contResp.content : '';
            console.log(`üìé [CHAT_CONTINUE] Received chunk ${cont} length=${chunk.length}`);
            if (chunk) {
                fullContent += (fullContent.endsWith('\n') ? '' : '\n') + chunk;
            }
            response = contResp;
        }

        // Format response for frontend
        const chatResponse = {
            success: true,
            message: fullContent,
            model: response.model,
            usage: response.usage,
            timestamp: new Date().toISOString(),
            mode: mode || 'default',
            citations,
            sourceAttribution,
            debug: {
                searchResultsCount: searchResults.length,
                avgScore: searchResults.length > 0 ? searchResults.reduce((sum, result) => sum + (result.score || 0), 0) / searchResults.length : 0,
                maxScore: searchResults.length > 0 ? Math.max(...searchResults.map(result => result.score || 0)) : 0,
                documentTypes: searchResults.map(r => ({ fileName: r.fileName, documentType: r.documentType, type: r.type, score: r.score }))
            },
            retrieval: {
                mode: isAdditive ? 'additive' : 'single',
                lectureNames
            }
        };

        console.log(`‚úÖ Chat response sent successfully`);

        res.json(chatResponse);

    } catch (error) {
        console.error('‚ùå Error in chat endpoint:', error);

        // Provide user-friendly error messages
        let errorMessage = 'Sorry, I encountered an error processing your message.';
        let statusCode = 500;

        if (error.message.includes('OLLAMA_ENDPOINT')) {
            errorMessage = 'Ollama service is not available. Please check if Ollama is running.';
            statusCode = 503;
        } else if (error.message.includes('API key')) {
            errorMessage = 'Authentication error. Please check your API configuration.';
            statusCode = 401;
        } else if (error.message.includes('endpoint')) {
            errorMessage = 'Service endpoint is not reachable. Please check your configuration.';
            statusCode = 503;
        }

        res.status(statusCode).json({
            success: false,
            message: errorMessage,
            error: process.env.NODE_ENV === 'development' ? error.message : undefined,
            timestamp: new Date().toISOString()
        });
    }
});


/**
 * GET /api/chat/status
 * Get the current status of the LLM service
 */
router.get('/status', async (req, res) => {
    try {
        const llmService = req.app.locals.llm;

        if (!llmService) {
            return res.status(503).json({
                success: false,
                message: 'LLM service is not initialized'
            });
        }

        const status = llmService.getStatus();

        res.json({
            success: true,
            data: status
        });

    } catch (error) {
        console.error('‚ùå Error getting chat status:', error);

        res.status(500).json({
            success: false,
            message: 'Failed to get chat status',
            error: process.env.NODE_ENV === 'development' ? error.message : undefined
        });
    }
});



/**
 * POST /api/chat/test
 * Test the LLM connection
 */
router.post('/test', async (req, res) => {
    try {
        console.log('üß™ Testing LLM connection...');

        const llmService = req.app.locals.llm;

        if (!llmService) {
            return res.status(503).json({
                success: false,
                message: 'LLM service is not initialized'
            });
        }

        const isConnected = await llmService.testConnection();

        if (isConnected) {
            res.json({
                success: true,
                message: 'LLM connection test successful',
                provider: llmService.getProviderName(),
                timestamp: new Date().toISOString()
            });
        } else {
            res.status(503).json({
                success: false,
                message: 'LLM connection test failed',
                provider: llmService.getProviderName(),
                timestamp: new Date().toISOString()
            });
        }

    } catch (error) {
        console.error('‚ùå Error testing LLM connection:', error);

        res.status(500).json({
            success: false,
            message: 'Failed to test LLM connection',
            error: process.env.NODE_ENV === 'development' ? error.message : undefined,
            timestamp: new Date().toISOString()
        });
    }
});

/**
 * GET /api/chat/models
 * Get available models from the current provider
 */
router.get('/models', async (req, res) => {
    try {
        const llmService = req.app.locals.llm;

        if (!llmService) {
            return res.status(503).json({
                success: false,
                message: 'LLM service is not initialized'
            });
        }

        const models = await llmService.getAvailableModels();

        res.json({
            success: true,
            data: {
                models: models,
                provider: llmService.getProviderName(),
                timestamp: new Date().toISOString()
            }
        });

    } catch (error) {
        console.error('‚ùå Error getting available models:', error);

        res.status(500).json({
            success: false,
            message: 'Failed to get available models',
            error: process.env.NODE_ENV === 'development' ? error.message : undefined
        });
    }
});

/**
 * POST /api/chat/save
 * Save a chat session to the database for instructor access
 */
router.post('/save', async (req, res) => {
    try {
        const {
            sessionId,
            courseId,
            studentId,
            studentName,
            unitName,
            title,
            messageCount,
            duration,
            savedAt,
            chatData
        } = req.body;

        // Validate required fields
        if (!sessionId || !courseId || !studentId || !studentName) {
            return res.status(400).json({
                success: false,
                message: 'Missing required fields: sessionId, courseId, studentId, studentName'
            });
        }

        // Get database instance from app.locals
        const db = req.app.locals.db;
        if (!db) {
            return res.status(503).json({
                success: false,
                message: 'Database connection not available'
            });
        }

        // Save chat session to database
        const chatSessionsCollection = db.collection('chat_sessions');

        const sessionData = {
            sessionId,
            courseId,
            studentId,
            studentName,
            unitName: unitName || 'Unknown Unit',
            title: title || `Chat Session ${new Date().toLocaleDateString()}`,
            messageCount: messageCount || 0,
            duration: duration || 'Unknown',
            savedAt: savedAt || new Date().toISOString(),
            chatData: chatData || {},
            isDeleted: false, // Soft deletion flag
            createdAt: new Date()
        };

        // Insert or update the session
        await chatSessionsCollection.replaceOne(
            { sessionId: sessionId },
            sessionData,
            { upsert: true }
        );

        console.log(`Chat session saved: ${sessionId} for student ${studentName} in course ${courseId}`);

        res.json({
            success: true,
            message: 'Chat session saved successfully',
            data: {
                sessionId: sessionId,
                courseId: courseId,
                studentId: studentId
            }
        });

    } catch (error) {
        console.error('Error saving chat session:', error);
        res.status(500).json({
            success: false,
            message: 'Internal server error while saving chat session'
        });
    }
});

module.exports = router;