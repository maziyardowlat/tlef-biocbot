/**
 * Chat API Routes
 * Handles chat functionality with real LLM integration
 * Replaces mock responses with actual AI-powered responses
 */

const express = require('express');
const router = express.Router();
const LLMService = require('../services/llm');
const QdrantService = require('../services/qdrantService');
const prompts = require('../services/prompts');
const CourseModel = require('../models/Course');
const profanityCleaner = require('profanity-cleaner');
const TrackerService = require('../services/tracker');
const User = require('../models/User');

// Initialize services
// Initialize services lazily
let localTrackerService;

/**
 * Determine source attribution based on retrieved chunks
 * @param {Array} searchResults - Array of search results from Qdrant
 * @param {string} courseId - Course ID
 * @param {string} unitName - Current unit name
 * @param {Object} db - Database instance
 * @returns {Promise<Object>} Source attribution information
 */
/**
 * Determine source attribution based on retrieved chunks
 * @param {Array} searchResults - Array of search results from Qdrant
 * @param {string} courseId - Course ID
 * @param {string} unitName - Current unit name
 * @param {Object} db - Database instance
 * @returns {Promise<Object>} Source attribution information
 */
async function determineSourceAttribution(searchResults, courseId, unitName, db) {
    try {
        // If no search results, content is from GPT
        if (!searchResults || searchResults.length === 0) {
            return {
                source: 'GPT',
                description: 'Generated by AI (no relevant course materials found)',
                unitName: null,
                documentType: null
            };
        }

        // Check if search results are relevant enough (low scores indicate poor matches)
        const avgScore = searchResults.reduce((sum, result) => sum + (result.score || 0), 0) / searchResults.length;
        const maxScore = Math.max(...searchResults.map(result => result.score || 0));

        console.log('üîç [SOURCE_DEBUG] Search relevance - avgScore:', avgScore, 'maxScore:', maxScore);

        // If scores are too low, treat as GPT (no relevant materials found)
        // Very low thresholds to match actual score ranges (0.05-0.4 range)
        // If scores are too low, treat as GPT (no relevant materials found)
        // Raised thresholds to reduce false positive attributions (e.g. diamonds != carbon notes)
        if (avgScore < 0.15 || maxScore < 0.25) {
            console.log('üîç [SOURCE_DEBUG] Low relevance scores - treating as GPT');
            return {
                source: 'GPT',
                description: 'Generated by AI (no relevant course materials found)',
                unitName: null,
                documentType: null
            };
        }

        // Filter for meaningful chunks
        const relevantChunks = searchResults.filter(result => (result.score || 0) > 0.15);

        if (relevantChunks.length === 0) {
             return {
                source: 'GPT',
                description: 'Generated by AI (no relevant course materials found)',
                unitName: null,
                documentType: null
            };
        }

        // Collect unique sources
        const sources = new Set();
        
        relevantChunks.forEach(chunk => {
            const docType = chunk.type || chunk.documentType || 'unknown';
            const sourceUnit = chunk.lectureName || unitName;
            
            let readableType = 'Material';
            switch (docType) {
                case 'lecture_notes': readableType = 'Lecture Notes'; break;
                case 'practice-quiz': readableType = 'Practice Questions'; break;
                case 'additional': readableType = 'Additional Materials'; break;
                default: readableType = 'Course Material';
            }

            sources.add(`${readableType} (${sourceUnit})`);
        });

        // Debug: Log details about practice quiz chunks specifically
        const practiceQuizChunks = searchResults.filter(r => r.type === 'practice_q_tutorials' || r.documentType === 'practice-quiz');
        if (practiceQuizChunks.length > 0) {
            console.log('üîç [SOURCE_DEBUG] Practice quiz chunks found:', practiceQuizChunks.length);
        }

        const sourceDescription = Array.from(sources).join(', ');
        console.log('üîç [SOURCE_DEBUG] Final Source Description:', sourceDescription);

        return {
            source: 'multiple',
            description: `From: ${sourceDescription}`,
            unitName: unitName,
            documentType: 'multiple'
        };

    } catch (error) {
        console.error('Error determining source attribution:', error);
        return {
            source: 'GPT',
            description: 'Generated by AI (error determining source)',
            unitName: null,
            documentType: null
        };
    }
}

/**
 * Analyze chunk sources to determine document types
 * @param {Array} searchResults - Array of search results
 * @returns {Object} Analysis of chunk sources
 */
function analyzeChunkSources(searchResults) {
    const analysis = {
        hasLectureNotes: false,
        hasAdditionalMaterials: false,
        hasPracticeQuiz: false,
        hasUnknownType: false
    };

    for (const result of searchResults) {
        const docType = result.type || result.documentType || 'unknown';
        console.log('üîç [SOURCE_DEBUG] Analyzing chunk with docType:', docType, 'fileName:', result.fileName);

        switch (docType) {
            case 'lecture_notes':
                analysis.hasLectureNotes = true;
                break;
            case 'additional':
                analysis.hasAdditionalMaterials = true;
                break;
            case 'practice_q_tutorials':
                analysis.hasPracticeQuiz = true;
                break;
            case 'unknown':
                analysis.hasUnknownType = true;
                // Try to infer type from filename for legacy documents
                const fileName = (result.fileName || '').toLowerCase();
                console.log('üîç [SOURCE_DEBUG] Inferring type from filename:', fileName);

                // Be more specific about lecture notes inference
                if (fileName.includes('lecture') && (fileName.includes('notes') || fileName.includes('note'))) {
                    analysis.hasLectureNotes = true;
                } else if (fileName.includes('practice') || fileName.includes('quiz') || fileName.includes('tutorial')) {
                    analysis.hasPracticeQuiz = true;
                } else {
                    // Default to additional materials for unknown types
                    analysis.hasAdditionalMaterials = true;
                }
                break;
        }
    }

    return analysis;
}

/**
 * Check if retrieved chunks match learning objectives content
 * @param {Array} searchResults - Array of search results
 * @param {string} courseId - Course ID
 * @param {string} unitName - Unit name
 * @param {Object} db - Database instance
 * @returns {Promise<boolean>} True if chunks match learning objectives
 */
async function checkLearningObjectivesMatch(searchResults, courseId, unitName, db) {
    try {
        // Get learning objectives for the unit
        const learningObjectives = await CourseModel.getLearningObjectives(db, courseId, unitName);

        if (!learningObjectives || learningObjectives.length === 0) {
            return false;
        }

        // Check if any chunk text contains learning objective keywords
        const objectiveKeywords = learningObjectives.flatMap(obj =>
            obj.toLowerCase().split(/\s+/).filter(word => word.length > 3)
        );

        for (const result of searchResults) {
            const chunkText = result.chunkText.toLowerCase();
            const matches = objectiveKeywords.filter(keyword =>
                chunkText.includes(keyword)
            );

            // If chunk matches multiple learning objective keywords, likely from objectives
            if (matches.length >= 2) {
                return true;
            }
        }

        return false;
    } catch (error) {
        console.error('Error checking learning objectives match:', error);
        return false;
    }
}

// Middleware to parse JSON bodies
router.use(express.json());

/**
 * POST /api/chat
 * Send a message to the LLM and get a response
 */
router.post('/', async (req, res) => {
    try {
        console.log('üî• [CHAT_ROUTE_HIT] Processing POST /api/chat');
        console.log('üí¨ [CHAT_API] New chat request received');
        console.log('üí¨ [CHAT_API] Message:', req.body.message?.substring(0, 50) + '...');
        console.log('üí¨ [CHAT_API] Has conversationContext:', !!req.body.conversationContext);
        console.log('üîê [CHAT_API] Auth check - Cookie present:', !!req.headers.cookie);
        console.log('üîê [CHAT_API] Auth check - User present:', !!req.user);

        const llmService = req.app.locals.llm;

        // Check if LLM service is initialized
        if (!llmService) {
            return res.status(503).json({
                success: false,
                message: 'LLM service is not yet initialized. Please try again in a moment.'
            });
        }

        const { message, conversationId, mode, unitName, courseId, conversationContext } = req.body;

        // Get DB connection early
        const db = req.app.locals.db;
        if (!db) {
            return res.status(503).json({ success: false, message: 'Database connection not available' });
        }


        // Validate required fields
        if (!message || typeof message !== 'string') {
            return res.status(400).json({
                success: false,
                message: 'Message is required and must be a string'
            });
        }

        console.log(`üí¨ Chat request received: "${message.substring(0, 50)}..."`);
        console.log(`üéØ Mode: ${mode || 'default'}`);

        // Require courseId and unitName per requirements
        if (!courseId || !unitName) {
            return res.status(400).json({
                success: false,
                message: 'courseId and unitName are required to start chat'
            });
        }

        // Profanity filter: intercept before any RAG/GPT work to save tokens
        // If the cleaned text differs from the original, return a warning response
        // Skip for explanation requests (system generated from valid bot content)
        if (!req.body.isExplanationRequest) {
            const cleanedMessage = profanityCleaner && typeof profanityCleaner.clean === 'function'
                ? profanityCleaner.clean(message)
                : message;

            if (cleanedMessage !== message) {
            const warningText = 'please watch the language, this is a tool and this data will be used for internal and data analysis';
            console.log('‚ö†Ô∏è [CHAT_API] Profanity detected. Returning warning without querying LLM.');
            return res.json({
                success: true,
                message: warningText,
                model: 'system',
                usage: { tokens: 0 },
                timestamp: new Date().toISOString(),
                mode: mode || 'default',
                citations: [],
                sourceAttribution: {
                    source: 'system',
                    description: 'Profanity filtered warning',
                    unitName: null,
                    documentType: null
                },
                debug: {
                    profanityFiltered: true
                },
                retrieval: {
                    mode: 'n/a',
                    lectureNames: []
                }

            });
        }
    }
        
        
        // Safety/Wellness Check
        const safetyKeywords = ['suicide', 'kill myself', 'want to die', 'end my life', 'ending it all'];
        const lowerMessage = message.toLowerCase();
        if (safetyKeywords.some(keyword => lowerMessage.includes(keyword))) {
            console.log('‚ö†Ô∏è [CHAT_API] Safety keyword detected. Returning wellness resource.');
            return res.json({
                success: true,
                message: "I'm very sorry you're feeling this way. I'm an AI study assistant and not equipped to provide the support you need, but please reach out for help. The UBC Wellness Centre is available for you: http://students.ubc.ca/health/wellness-centre/",
                model: 'system',
                usage: { tokens: 0 },
                timestamp: new Date().toISOString(),
                mode: mode || 'default',
                citations: [],
                sourceAttribution: {
                    source: 'system',
                    description: 'Wellness Resource',
                    unitName: null,
                    documentType: null
                },
                debug: {
                    safetyTriggered: true
                },
                retrieval: {
                    mode: 'n/a',
                    lectureNames: []
                },
                struggleState: null
            });
        }

        // STRUGGLE DETECTION & TRACKING
        let struggleState = null;
        let directiveModeActive = false;
        let identifiedTopic = null;

        // Initialize TrackerService if needed
        const appLLM = req.app.locals.llm;
        if (appLLM && !localTrackerService) {
            localTrackerService = new TrackerService(appLLM);
            console.log('‚úÖ [CHAT_API_DEBUG] Local TrackerService initialized from app.locals.llm');
        }

        console.log(`üïµÔ∏è [CHAT_API_DEBUG] User Context: ID=${req.user ? req.user.userId : 'MISSING'}, Tracker=${!!localTrackerService}`);

        if (req.user && localTrackerService) {
            try {
                // Check if this is an explanation request with a known topic
                // If so, we can skip analysis and directly increment struggle for that topic
                if (req.body.isExplanationRequest && req.body.topic) {
                     console.log(`üïµÔ∏è [CHAT_API_DEBUG] Explanation Request for topic: "${req.body.topic}" - Incrementing Struggle Count`);
                     
                     const updateResult = await User.updateUserStruggleState(db, req.user.userId, {
                        topic: req.body.topic,
                        isStruggling: true,
                        reason: 'User requested explanation'
                     }, courseId);
                     
                     console.log('üïµÔ∏è [CHAT_API_DEBUG] Struggle Update (Explain) Result:', JSON.stringify(updateResult, null, 2));

                     if (updateResult.success) {
                        struggleState = updateResult.state; // topic specific state
                        identifiedTopic = req.body.topic;
                        
                        // We also need the full state to determine if ANY directive mode is active
                        // But updateUserStruggleState returns { success, state, allTopics }
                        // Let's verify if *this* topic triggered directive mode
                         if (struggleState && struggleState.isActive) {
                            directiveModeActive = true;
                            console.log(`üö® [CHAT_API_DEBUG] Directive Mode ACTIVATED via Explain for topic: ${identifiedTopic}`);
                        }
                    }

                } else {
                    // Normal message analysis
                    // 1. Analyze message for struggle
                    console.log('üïµÔ∏è [CHAT_API_DEBUG] ------------------------------------------------');
                    console.log(`üïµÔ∏è [CHAT_API_DEBUG] Analysis Start for msg: "${message.substring(0, 50)}..."`);
                    
                    const analysis = await localTrackerService.analyzeMessage(message, courseId, unitName);
                    console.log('üïµÔ∏è [CHAT_API_DEBUG] Raw Analysis Result:', JSON.stringify(analysis, null, 2));
                    
                    // 2. Update user state and persist to MongoDB
                    const updateResult = await User.updateUserStruggleState(db, req.user.userId, analysis, courseId);
                    console.log('üïµÔ∏è [CHAT_API_DEBUG] User State Update Result:', JSON.stringify(updateResult, null, 2));
                    
                    if (updateResult.success) {
                        struggleState = updateResult.state;
                        identifiedTopic = analysis.topic;
                        
                        // 3. Check if Directive Mode should be active
                        if (struggleState && struggleState.isActive) {
                            directiveModeActive = true;
                            console.log(`üö® [CHAT_API_DEBUG] Directive Mode ACTIVATED for topic: ${identifiedTopic}`);
                        } else {
                             console.log(`üïµÔ∏è [CHAT_API_DEBUG] Struggle recorded but Directive Mode NOT active yet.`);
                        }
                    }
                }
            } catch (trackerError) {
                console.error('‚ùå [CHAT_API_DEBUG] Error in struggle tracking:', trackerError);
            }
        } else {
            console.warn('‚ö†Ô∏è [CHAT_API_DEBUG] Check skipped. User:', !!req.user, 'Tracker:', !!localTrackerService);
            if (!req.user) console.warn('‚ö†Ô∏è [CHAT_API_DEBUG] req.user is MISSING. Auth middleware might be failing.');
            if (!localTrackerService) console.warn('‚ö†Ô∏è [CHAT_API_DEBUG] localTrackerService is MISSING. LLM service might not be ready.');
        }


        // Initialize Qdrant
        let qdrant;
        try {
            qdrant = new QdrantService();
            await qdrant.initialize();
        } catch (qdrantError) {
            console.error('‚ùå Qdrant initialization failed:', qdrantError.message);
            // Check if it's a connection error
            if (qdrantError.message.includes('ECONNREFUSED') || qdrantError.message.includes('fetch failed')) {
                return res.status(503).json({
                    success: false,
                    message: 'Qdrant vector database is not available. Please ensure Qdrant is running.',
                    error: 'QDRANT_CONNECTION_FAILED',
                    instructions: 'Start Qdrant with: docker run -p 6333:6333 qdrant/qdrant'
                });
            }
            // Other Qdrant errors
            return res.status(503).json({
                success: false,
                message: 'Vector database service error',
                error: qdrantError.message
            });
        }

        // Load course to get retrieval mode and published lectures
        const coursesCol = db.collection('courses');
        const course = await coursesCol.findOne({ courseId });
        if (!course) {
            return res.status(404).json({ success: false, message: 'Course not found' });
        }

        // (Global settings fetch removed - prompts now come from course document)

        // Determine retrieval mode: Course override ? Default : False (default)
        const isAdditive = course.isAdditiveRetrieval !== undefined && course.isAdditiveRetrieval !== null
            ? !!course.isAdditiveRetrieval 
            : false;

        // Build lectureNames filter using published units only, ordered by lectures array
        const publishedLectures = (course.lectures || []).filter(l => l.isPublished).map(l => l.name);
        if (!publishedLectures.includes(unitName)) {
            return res.status(400).json({ success: false, message: 'Selected unit is not published or does not exist' });
        }

        let lectureNames = [unitName];
        if (isAdditive) {
            const order = (course.lectures || []).filter(l => l.isPublished).map(l => l.name);
            const idx = order.indexOf(unitName);
            lectureNames = idx >= 0 ? order.slice(0, idx + 1) : [unitName];
        }

        // Debug logging to verify retrieval mode and scope
        console.log(`üîé [CHAT_RAG] RetrievalMode=${isAdditive ? 'additive' : 'single'} | Course=${courseId} | Unit=${unitName} | LectureNames=${JSON.stringify(lectureNames)}`);

        // Retrieve top chunks from Qdrant
        const searchResults = await qdrant.searchDocuments(message, { courseId, lectureNames }, 12);

        // Log summary of results by lecture to validate scope
        try {
            const lecturesHit = Array.from(new Set((searchResults || []).map(r => r.lectureName)));
            console.log(`üìö [CHAT_RAG] Retrieved ${searchResults.length} chunks from lectures: ${lecturesHit.join(', ')}`);
            // Group by document to see which files contributed
            const byDoc = {};
            for (const r of (searchResults || [])) {
                const docId = r.documentId || 'unknown-doc';
                if (!byDoc[docId]) {
                    byDoc[docId] = {
                        fileName: r.fileName || 'unknown-filename',
                        lectures: new Set(),
                        count: 0,
                        maxScore: 0
                    };
                }
                byDoc[docId].count += 1;
                byDoc[docId].lectures.add(r.lectureName);
                if (typeof r.score === 'number' && r.score > byDoc[docId].maxScore) {
                    byDoc[docId].maxScore = r.score;
                }
            }
            const docKeys = Object.keys(byDoc);
            console.log(`üìÑ [CHAT_RAG] Documents contributing (${docKeys.length}):`);
            for (const k of docKeys) {
                const info = byDoc[k];
                const lecturesList = Array.from(info.lectures).join(', ');
                const scoreStr = info.maxScore ? info.maxScore.toFixed(3) : 'n/a';
                console.log(`   - ${info.fileName} (id=${k}) | lectures=[${lecturesList}] | chunks=${info.count} | maxScore=${scoreStr}`);
            }
        } catch (_) {}

        // Build concise context window with citations
        const citations = searchResults.map(r => ({
            lectureName: r.lectureName,
            fileName: r.fileName,
            score: r.score
        }));
        const contextText = searchResults
            .map(r => `From ${r.lectureName} (${r.fileName}):\n${r.chunkText}`)
            .join('\n\n---\n\n');

        // Determine source attribution based on retrieved chunks
        console.log('üîç [SOURCE_DEBUG] Retrieved chunks for source analysis:', searchResults.map(r => ({
            fileName: r.fileName,
            documentType: r.documentType,
            type: r.type,
            lectureName: r.lectureName
        })));

        let sourceAttribution;
        try {
            sourceAttribution = await determineSourceAttribution(searchResults, courseId, unitName, db);
            console.log('üîç [SOURCE_DEBUG] Determined source attribution:', sourceAttribution);
        } catch (error) {
            console.error('üîç [SOURCE_DEBUG] Error in source attribution:', error);
            sourceAttribution = {
                source: 'GPT',
                description: 'Generated by AI (error determining source)',
                unitName: null,
                documentType: null
            };
        }

        // Build the message to send to LLM
        let messageToSend;
        
        if (mode === 'protege') {
            messageToSend = `
CONTEXT (The correct answers):
${contextText}

INSTRUCTIONS:
Based on the Context above, act as the student described in the System Prompt.
The user just said: "${message}"
Do not explain the context to the user. Ask the user to explain it to you.`;
        } else if (req.body.isExplanationRequest) {
            // Explain mode
            messageToSend = `Use the provided course context to help explain the concept if needed.
\n\nCourse context:\n${contextText}\n\nConcept/Text to explain:\n"${message}"`;
        } else {
            // Default/Tutor mode
            messageToSend = `Use only the provided course context to answer. Cite which unit a fact came from.
\n\nCourse context:\n${contextText}\n\nStudent question: ${message}`;
        }

        // If we have conversation context (continuing a chat), use structured conversation approach
        if (conversationContext && conversationContext.conversationMessages) {
            console.log('üîÑ [CHAT_CONTINUE] Using structured conversation context');

            // Build the conversation history as a single message
            let conversationHistory = '';
            conversationContext.conversationMessages.forEach(msg => {
                const speaker = msg.role === 'user' ? 'Student' : 'BiocBot';
                conversationHistory += `${speaker}: ${msg.content}\n\n`;
            });

            // Add the student's new message
            conversationHistory += `Student: ${message}`;

            // Create the full message with conversation context
            messageToSend = `Use only the provided course context to answer. Cite which unit a fact came from.

Course context:
${contextText}

Previous conversation:
${conversationHistory}`;
        }

        // Retrieve custom prompts from course object or use defaults
        let basePrompt = prompts.DEFAULT_PROMPTS.base;
        let protegePrompt = prompts.DEFAULT_PROMPTS.protege;
        let tutorPrompt = prompts.DEFAULT_PROMPTS.tutor;
        let explainPrompt = prompts.DEFAULT_PROMPTS.explain;
        let directivePrompt = prompts.DEFAULT_PROMPTS.directive;

        // Check if course has custom prompts
        if (course.prompts) {
            console.log('üìù [CHAT_API] Using course-specific prompts');
            if (course.prompts.base) basePrompt = course.prompts.base;
            if (course.prompts.protege) protegePrompt = course.prompts.protege;
            if (course.prompts.tutor) tutorPrompt = course.prompts.tutor;
            if (course.prompts.explain) explainPrompt = course.prompts.explain;
            if (course.prompts.directive) directivePrompt = course.prompts.directive;
        } else {
            console.log('[CHAT_API] Using default prompts');
        }

        // Apply Directive Mode adjustments if active
        // Apply Directive Mode adjustments if active
        if (directiveModeActive) {
            // Locked Trigger: Always prepend the topic announcement
            const lockedHeader = `\n\nCRITICAL INSTRUCTION: The student is struggling significantly with the topic "${identifiedTopic}".\nSwitch to DIRECTIVE MODE:\n`;
            
            // Configurable Strategy: Append the instructor's custom instructions
            // Append to TUTOR prompt only to avoid polluting Protege mode or Base
            tutorPrompt += lockedHeader + directivePrompt;
            
            console.log(' [CHAT_API] Appended Directive Mode instructions (Header + Strategy) to TUTOR prompt');
        }

        // Check for summary attempt via LLM if requested
        let shouldAppendReprompt = false;
        if (req.body.checkSummaryAttempt) {
            try {
                console.log('üîç [SUMMARY_CHECK] Analyzing student message for summary attempt...');
                const summaryCheckPrompt = `Analyze the following student message. Does it attempt to summarize, recap, or explain the previous conversation? Respond with only YES or NO. Message: "${message}"`;
                
                // Use a separate, cheap LLM call (low temp, system prompt irrelevant but using base for safety)
                const summaryCheckResponse = await llmService.sendMessage(summaryCheckPrompt, {
                    temperature: 0.1,
                    maxTokens: 10,
                    systemPrompt: "You are a classifier. Respond only with YES or NO."
                });

                const isSummary = summaryCheckResponse && summaryCheckResponse.content && summaryCheckResponse.content.trim().toUpperCase().includes('YES');
                console.log(`üîç [SUMMARY_CHECK] Result: ${isSummary ? 'YES' : 'NO'} (Raw: ${summaryCheckResponse?.content})`);

                if (!isSummary) {
                    shouldAppendReprompt = true;
                    console.log('üîç [SUMMARY_CHECK] Student did NOT summarize -> Appending re-prompt');
                } else {
                    console.log('üîç [SUMMARY_CHECK] Student provided summary -> No re-prompt');
                }
            } catch (err) {
                console.error('‚ùå [SUMMARY_CHECK] Error during check:', err);
                // Fail safe: don't annoy the user if check fails. Or default to true? 
                // Let's default to false (no re-prompt) to be safe.
            }
        }

        // In the future, we can implement conversation persistence
        let response = await llmService.sendMessage(
            messageToSend,
            {
            // Adjust response based on student mode
            temperature: mode === 'protege' ? 0.5 : 0.5,
            maxTokens: mode === 'protege' ? 32768 : 32768,
            systemPrompt: basePrompt +
                (req.body.isExplanationRequest ? explainPrompt : 
                (mode === 'protege' ? protegePrompt : tutorPrompt))
        });

        let fullContent = response && response.content ? response.content : '';

        // Detect truncation and auto-continue up to N times
        function extractFinishReason(resp) {
            try {
                return (resp && (resp.finishReason || resp.finish_reason || (resp.usage && resp.usage.finish_reason) || resp.stopReason || resp.stop_reason)) || '';
            } catch (e) { return ''; }
        }
        function isLikelyTruncated(resp, content) {
            const fr = (extractFinishReason(resp) + '').toLowerCase();
            if (fr.includes('length') || fr.includes('token')) return true;
            if (!content) return false;
            const tail = content.slice(-60);
            const endsClean = /([\.\!\?]|\(Unit\s+[^)]+\))\s*$/i.test(tail);
            return !endsClean && content.length > 300;
        }

        const MAX_CONTINUATIONS = 2;
        let cont = 0;
        while (cont < MAX_CONTINUATIONS && isLikelyTruncated(response, fullContent)) {
            cont += 1;
            console.log(`‚è© [CHAT_CONTINUE] Requesting continuation ${cont}; current length=${fullContent.length}`);
            const tailSnippet = fullContent.slice(-200);
            const contPrompt = `Continue the previous answer. Do not repeat earlier content. Pick up seamlessly from here: "${tailSnippet}"`;
            const contResp = await llmService.sendMessage(contPrompt, {
                temperature: mode === 'protege' ? 0.8 : 0.6,
                maxTokens: mode === 'protege' ? 32768 : 32768,
                systemPrompt: basePrompt +
                    (mode === 'protege' ? protegePrompt : tutorPrompt)
            });
            const chunk = contResp && contResp.content ? contResp.content : '';
            console.log(`üìé [CHAT_CONTINUE] Received chunk ${cont} length=${chunk.length}`);
            if (chunk) {
                fullContent += (fullContent.endsWith('\n') ? '' : '\n') + chunk;
            }
            response = contResp;
        }

        // Append the re-prompt if the summary check determined it was needed
        if (shouldAppendReprompt) {
            fullContent += '\n\n----------------\nHey, I know you asked another question, would you like to summarize our chat again?';
        }

        // Format response for frontend
        const chatResponse = {
            success: true,
            message: fullContent,
            model: response.model,
            usage: response.usage,
            timestamp: new Date().toISOString(),
            mode: mode || 'default',
            citations,
            sourceAttribution,
            debug: {
                searchResultsCount: searchResults.length,
                avgScore: searchResults.length > 0 ? searchResults.reduce((sum, result) => sum + (result.score || 0), 0) / searchResults.length : 0,
                maxScore: searchResults.length > 0 ? Math.max(...searchResults.map(result => result.score || 0)) : 0,
                documentTypes: searchResults.map(r => ({ fileName: r.fileName, documentType: r.documentType, type: r.type, score: r.score }))
            },
            retrieval: {
                mode: isAdditive ? 'additive' : 'single',
                lectureNames
            },
            struggleState: struggleState,
            struggleDebug: {
                userExists: !!req.user,
                userId: req.user ? req.user.userId : null,
                trackerInitialized: !!localTrackerService,
                directiveModeActive: directiveModeActive,
                identifiedTopic: identifiedTopic
            }
        };

        console.log(`‚úÖ Chat response sent successfully`);

        res.json(chatResponse);

    } catch (error) {
        console.error('‚ùå Error in chat endpoint:', error);

        // Provide user-friendly error messages
        let errorMessage = 'Sorry, I encountered an error processing your message.';
        let statusCode = 500;

        if (error.message.includes('OLLAMA_ENDPOINT')) {
            errorMessage = 'Ollama service is not available. Please check if Ollama is running.';
            statusCode = 503;
        } else if (error.message.includes('API key')) {
            errorMessage = 'Authentication error. Please check your API configuration.';
            statusCode = 401;
        } else if (error.message.includes('endpoint')) {
            errorMessage = 'Service endpoint is not reachable. Please check your configuration.';
            statusCode = 503;
        }

        res.status(statusCode).json({
            success: false,
            message: errorMessage,
            error: process.env.NODE_ENV === 'development' ? error.message : undefined,
            timestamp: new Date().toISOString()
        });
    }
});


/**
 * GET /api/chat/status
 * Get the current status of the LLM service
 */
router.get('/status', async (req, res) => {
    try {
        const llmService = req.app.locals.llm;

        if (!llmService) {
            return res.status(503).json({
                success: false,
                message: 'LLM service is not initialized'
            });
        }

        const status = llmService.getStatus();

        res.json({
            success: true,
            data: status
        });

    } catch (error) {
        console.error('‚ùå Error getting chat status:', error);

        res.status(500).json({
            success: false,
            message: 'Failed to get chat status',
            error: process.env.NODE_ENV === 'development' ? error.message : undefined
        });
    }
});



/**
 * POST /api/chat/test
 * Test the LLM connection
 */
router.post('/test', async (req, res) => {
    try {
        console.log('üß™ Testing LLM connection...');

        const llmService = req.app.locals.llm;

        if (!llmService) {
            return res.status(503).json({
                success: false,
                message: 'LLM service is not initialized'
            });
        }

        const isConnected = await llmService.testConnection();

        if (isConnected) {
            res.json({
                success: true,
                message: 'LLM connection test successful',
                provider: llmService.getProviderName(),
                timestamp: new Date().toISOString()
            });
        } else {
            res.status(503).json({
                success: false,
                message: 'LLM connection test failed',
                provider: llmService.getProviderName(),
                timestamp: new Date().toISOString()
            });
        }

    } catch (error) {
        console.error('‚ùå Error testing LLM connection:', error);

        res.status(500).json({
            success: false,
            message: 'Failed to test LLM connection',
            error: process.env.NODE_ENV === 'development' ? error.message : undefined,
            timestamp: new Date().toISOString()
        });
    }
});

/**
 * GET /api/chat/models
 * Get available models from the current provider
 */
router.get('/models', async (req, res) => {
    try {
        const llmService = req.app.locals.llm;

        if (!llmService) {
            return res.status(503).json({
                success: false,
                message: 'LLM service is not initialized'
            });
        }

        const models = await llmService.getAvailableModels();

        res.json({
            success: true,
            data: {
                models: models,
                provider: llmService.getProviderName(),
                timestamp: new Date().toISOString()
            }
        });

    } catch (error) {
        console.error('‚ùå Error getting available models:', error);

        res.status(500).json({
            success: false,
            message: 'Failed to get available models',
            error: process.env.NODE_ENV === 'development' ? error.message : undefined
        });
    }
});

/**
 * POST /api/chat/save
 * Save a chat session to the database for instructor access
 */
router.post('/save', async (req, res) => {
    try {
        const {
            sessionId,
            courseId,
            studentId,
            studentName,
            unitName,
            title,
            messageCount,
            duration,
            savedAt,
            chatData
        } = req.body;

        // Validate required fields
        if (!sessionId || !courseId || !studentId || !studentName) {
            return res.status(400).json({
                success: false,
                message: 'Missing required fields: sessionId, courseId, studentId, studentName'
            });
        }

        // Get database instance from app.locals
        const db = req.app.locals.db;
        if (!db) {
            return res.status(503).json({
                success: false,
                message: 'Database connection not available'
            });
        }

        // Save chat session to database
        const chatSessionsCollection = db.collection('chat_sessions');

        const sessionData = {
            sessionId,
            courseId,
            studentId,
            studentName,
            unitName: unitName || 'Unknown Unit',
            title: title || `Chat Session ${new Date().toLocaleDateString()}`,
            messageCount: messageCount || 0,
            duration: duration || 'Unknown',
            savedAt: savedAt || new Date().toISOString(),
            chatData: chatData || {},
            isDeleted: false, // Soft deletion flag
            createdAt: new Date()
        };

        // Insert or update the session
        await chatSessionsCollection.replaceOne(
            { sessionId: sessionId },
            sessionData,
            { upsert: true }
        );

        console.log(`Chat session saved: ${sessionId} for student ${studentName} in course ${courseId}`);

        res.json({
            success: true,
            message: 'Chat session saved successfully',
            data: {
                sessionId: sessionId,
                courseId: courseId,
                studentId: studentId
            }
        });

    } catch (error) {
        console.error('Error saving chat session:', error);
        res.status(500).json({
            success: false,
            message: 'Internal server error while saving chat session'
        });
    }
});

module.exports = router;