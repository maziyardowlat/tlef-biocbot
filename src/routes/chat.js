/**
 * Chat API Routes
 * Handles chat functionality with real LLM integration
 * Replaces mock responses with actual AI-powered responses
 */

const express = require('express');
const router = express.Router();
const LLMService = require('../services/llm');
const QdrantService = require('../services/qdrantService');
const prompts = require('../services/prompts');
const CourseModel = require('../models/Course');

// Initialize LLM service
let llmService;
(async () => {
    try {
        llmService = await LLMService.create();
        console.log('‚úÖ LLM service initialized successfully');
    } catch (error) {
        console.error('‚ùå Failed to initialize LLM service:', error);
    }
})();

/**
 * Determine source attribution based on retrieved chunks
 * @param {Array} searchResults - Array of search results from Qdrant
 * @param {string} courseId - Course ID
 * @param {string} unitName - Current unit name
 * @param {Object} db - Database instance
 * @returns {Promise<Object>} Source attribution information
 */
async function determineSourceAttribution(searchResults, courseId, unitName, db) {
    try {
        // If no search results, content is from GPT
        if (!searchResults || searchResults.length === 0) {
            return {
                source: 'GPT',
                description: 'Generated by AI (no relevant course materials found)',
                unitName: null,
                documentType: null
            };
        }

        // Check if search results are relevant enough (low scores indicate poor matches)
        const avgScore = searchResults.reduce((sum, result) => sum + (result.score || 0), 0) / searchResults.length;
        const maxScore = Math.max(...searchResults.map(result => result.score || 0));
        
        console.log('üîç [SOURCE_DEBUG] Search relevance - avgScore:', avgScore, 'maxScore:', maxScore);
        
        // If scores are too low, treat as GPT (no relevant materials found)
        // Very low thresholds to match actual score ranges (0.05-0.4 range)
        if (avgScore < 0.05 || maxScore < 0.08) {
            console.log('üîç [SOURCE_DEBUG] Low relevance scores - treating as GPT');
            return {
                source: 'GPT',
                description: 'Generated by AI (no relevant course materials found)',
                unitName: null,
                documentType: null
            };
        }

        // Additional check: if all chunks have very low scores, treat as GPT
        const relevantChunks = searchResults.filter(result => (result.score || 0) > 0.05);
        if (relevantChunks.length === 0) {
            console.log('üîç [SOURCE_DEBUG] No chunks with sufficient relevance - treating as GPT');
            return {
                source: 'GPT',
                description: 'Generated by AI (no relevant course materials found)',
                unitName: null,
                documentType: null
            };
        }

        // Analyze the retrieved chunks to determine primary source
        const sourceAnalysis = analyzeChunkSources(searchResults);
        console.log('üîç [SOURCE_DEBUG] Source analysis:', sourceAnalysis);
        
        // Debug: Log details about practice quiz chunks specifically
        const practiceQuizChunks = searchResults.filter(r => r.type === 'practice_q_tutorials' || r.documentType === 'practice-quiz');
        if (practiceQuizChunks.length > 0) {
            console.log('üîç [SOURCE_DEBUG] Practice quiz chunks found:', practiceQuizChunks.map(c => ({
                fileName: c.fileName,
                score: c.score,
                chunkPreview: c.chunkText.substring(0, 100) + '...'
            })));
        }
        
        // Check if any chunks are from learning objectives
        let hasLearningObjectives = false;
        try {
            hasLearningObjectives = await checkLearningObjectivesMatch(searchResults, courseId, unitName, db);
            console.log('üîç [SOURCE_DEBUG] Has learning objectives match:', hasLearningObjectives);
        } catch (error) {
            console.error('üîç [SOURCE_DEBUG] Error checking learning objectives:', error);
            hasLearningObjectives = false;
        }
        
        // Determine primary source based on the highest scoring document type
        // Find the document type with the highest average score
        const documentTypeScores = {};
        
        // Group results by document type and calculate average scores
        searchResults.forEach(result => {
            const docType = result.type || result.documentType || 'unknown';
            if (!documentTypeScores[docType]) {
                documentTypeScores[docType] = { scores: [], count: 0 };
            }
            documentTypeScores[docType].scores.push(result.score || 0);
            documentTypeScores[docType].count++;
        });
        
        // Calculate average scores for each document type
        Object.keys(documentTypeScores).forEach(docType => {
            const scores = documentTypeScores[docType].scores;
            documentTypeScores[docType].avgScore = scores.reduce((sum, score) => sum + score, 0) / scores.length;
        });
        
        console.log('üîç [SOURCE_DEBUG] Document type scores:', documentTypeScores);
        
        // Find the document type with the highest average score
        const highestScoringType = Object.keys(documentTypeScores).reduce((a, b) => 
            documentTypeScores[a].avgScore > documentTypeScores[b].avgScore ? a : b
        );
        
        const highestScore = documentTypeScores[highestScoringType].avgScore;
        
        console.log('üîç [SOURCE_DEBUG] Highest scoring document type:', highestScoringType);
        console.log('üîç [SOURCE_DEBUG] Highest score:', highestScore);
        
        // If the highest score is too low, treat as GPT
        if (highestScore < 0.1) {
            console.log('üîç [SOURCE_DEBUG] Highest score too low, treating as GPT');
            return {
                source: 'GPT',
                description: 'Generated by AI (no relevant course materials found)',
                unitName: null,
                documentType: null
            };
        }
        
        // Additional check: if the highest score is still relatively low, treat as GPT
        if (highestScore < 0.2) {
            console.log('üîç [SOURCE_DEBUG] Highest score still too low, treating as GPT');
            return {
                source: 'GPT',
                description: 'Generated by AI (no relevant course materials found)',
                unitName: null,
                documentType: null
            };
        }
        
        // Return source attribution based on the highest scoring document type
        switch (highestScoringType) {
            case 'lecture_notes':
                return {
                    source: 'lecture-notes',
                    description: `From lecture notes, ${unitName}`,
                    unitName: unitName,
                    documentType: 'lecture_notes'
                };
            case 'practice_q_tutorials':
                return {
                    source: 'practice-quiz',
                    description: 'From practice questions',
                    unitName: unitName,
                    documentType: 'practice_q_tutorials'
                };
            case 'additional':
                return {
                    source: 'additional-materials',
                    description: 'From additional materials',
                    unitName: unitName,
                    documentType: 'additional'
                };
            case 'readings':
                return {
                    source: 'readings',
                    description: 'From readings',
                    unitName: unitName,
                    documentType: 'readings'
                };
            case 'syllabus':
                return {
                    source: 'syllabus',
                    description: 'From syllabus',
                    unitName: unitName,
                    documentType: 'syllabus'
                };
            default:
                // Fallback to GPT if no clear source identified
                return {
                    source: 'GPT',
                    description: 'Generated by AI (no relevant course materials found)',
                    unitName: null,
                    documentType: null
                };
        }
        
        if (sourceAnalysis.hasReadings) {
            return {
                source: 'readings',
                description: 'From readings',
                unitName: unitName,
                documentType: 'readings'
            };
        }
        
        if (sourceAnalysis.hasSyllabus) {
            return {
                source: 'syllabus',
                description: 'From syllabus',
                unitName: unitName,
                documentType: 'syllabus'
            };
        }
        
        if (sourceAnalysis.hasAdditionalMaterials) {
            return {
                source: 'additional-materials',
                description: 'From additional materials',
                unitName: unitName,
                documentType: 'additional'
            };
        }
        
        // Fallback to GPT if no clear source identified
        return {
            source: 'GPT',
            description: 'Generated by AI (no relevant course materials found)',
            unitName: null,
            documentType: null
        };
        
    } catch (error) {
        console.error('Error determining source attribution:', error);
        return {
            source: 'GPT',
            description: 'Generated by AI (error determining source)',
            unitName: null,
            documentType: null
        };
    }
}

/**
 * Analyze chunk sources to determine document types
 * @param {Array} searchResults - Array of search results
 * @returns {Object} Analysis of chunk sources
 */
function analyzeChunkSources(searchResults) {
    const analysis = {
        hasLectureNotes: false,
        hasAdditionalMaterials: false,
        hasPracticeQuiz: false,
        hasReadings: false,
        hasSyllabus: false,
        hasUnknownType: false
    };
    
    for (const result of searchResults) {
        const docType = result.type || result.documentType || 'unknown';
        console.log('üîç [SOURCE_DEBUG] Analyzing chunk with docType:', docType, 'fileName:', result.fileName);
        
        switch (docType) {
            case 'lecture_notes':
                analysis.hasLectureNotes = true;
                break;
            case 'additional':
                analysis.hasAdditionalMaterials = true;
                break;
            case 'practice_q_tutorials':
                analysis.hasPracticeQuiz = true;
                break;
            case 'readings':
                analysis.hasReadings = true;
                break;
            case 'syllabus':
                analysis.hasSyllabus = true;
                break;
            case 'unknown':
                analysis.hasUnknownType = true;
                // Try to infer type from filename for legacy documents
                const fileName = (result.fileName || '').toLowerCase();
                console.log('üîç [SOURCE_DEBUG] Inferring type from filename:', fileName);
                
                // Be more specific about lecture notes inference
                if (fileName.includes('lecture') && (fileName.includes('notes') || fileName.includes('note'))) {
                    analysis.hasLectureNotes = true;
                } else if (fileName.includes('practice') || fileName.includes('quiz') || fileName.includes('tutorial')) {
                    analysis.hasPracticeQuiz = true;
                } else if (fileName.includes('reading') || fileName.includes('textbook')) {
                    analysis.hasReadings = true;
                } else if (fileName.includes('syllabus')) {
                    analysis.hasSyllabus = true;
                } else {
                    // Default to additional materials for unknown types
                    analysis.hasAdditionalMaterials = true;
                }
                break;
        }
    }
    
    return analysis;
}

/**
 * Check if retrieved chunks match learning objectives content
 * @param {Array} searchResults - Array of search results
 * @param {string} courseId - Course ID
 * @param {string} unitName - Unit name
 * @param {Object} db - Database instance
 * @returns {Promise<boolean>} True if chunks match learning objectives
 */
async function checkLearningObjectivesMatch(searchResults, courseId, unitName, db) {
    try {
        // Get learning objectives for the unit
        const learningObjectives = await CourseModel.getLearningObjectives(db, courseId, unitName);
        
        if (!learningObjectives || learningObjectives.length === 0) {
            return false;
        }
        
        // Check if any chunk text contains learning objective keywords
        const objectiveKeywords = learningObjectives.flatMap(obj => 
            obj.toLowerCase().split(/\s+/).filter(word => word.length > 3)
        );
        
        for (const result of searchResults) {
            const chunkText = result.chunkText.toLowerCase();
            const matches = objectiveKeywords.filter(keyword => 
                chunkText.includes(keyword)
            );
            
            // If chunk matches multiple learning objective keywords, likely from objectives
            if (matches.length >= 2) {
                return true;
            }
        }
        
        return false;
    } catch (error) {
        console.error('Error checking learning objectives match:', error);
        return false;
    }
}

// Middleware to parse JSON bodies
router.use(express.json());

/**
 * POST /api/chat
 * Send a message to the LLM and get a response
 */
router.post('/', async (req, res) => {
    try {
        console.log('üí¨ [CHAT_API] New chat request received');
        console.log('üí¨ [CHAT_API] Message:', req.body.message?.substring(0, 50) + '...');
        console.log('üí¨ [CHAT_API] Has conversationContext:', !!req.body.conversationContext);
        
        // Check if LLM service is initialized
        if (!llmService) {
            return res.status(503).json({
                success: false,
                message: 'LLM service is not yet initialized. Please try again in a moment.'
            });
        }

        const { message, conversationId, mode, unitName, courseId, conversationContext } = req.body;
        
        
        // Validate required fields
        if (!message || typeof message !== 'string') {
            return res.status(400).json({
                success: false,
                message: 'Message is required and must be a string'
            });
        }
        
        console.log(`üí¨ Chat request received: "${message.substring(0, 50)}..."`);
        console.log(`üéØ Mode: ${mode || 'default'}`);

        // Require courseId and unitName per requirements
        if (!courseId || !unitName) {
            return res.status(400).json({
                success: false,
                message: 'courseId and unitName are required to start chat'
            });
        }

        // Initialize Qdrant and DB
        const qdrant = new QdrantService();
        await qdrant.initialize();

        const db = req.app.locals.db;
        if (!db) {
            return res.status(503).json({ success: false, message: 'Database connection not available' });
        }

        // Load course to get retrieval mode and published lectures
        const coursesCol = db.collection('courses');
        const course = await coursesCol.findOne({ courseId });
        if (!course) {
            return res.status(404).json({ success: false, message: 'Course not found' });
        }

        const isAdditive = !!course.isAdditiveRetrieval;

        // Build lectureNames filter using published units only, ordered by lectures array
        const publishedLectures = (course.lectures || []).filter(l => l.isPublished).map(l => l.name);
        if (!publishedLectures.includes(unitName)) {
            return res.status(400).json({ success: false, message: 'Selected unit is not published or does not exist' });
        }

        let lectureNames = [unitName];
        if (isAdditive) {
            const order = (course.lectures || []).filter(l => l.isPublished).map(l => l.name);
            const idx = order.indexOf(unitName);
            lectureNames = idx >= 0 ? order.slice(0, idx + 1) : [unitName];
        }

        // Debug logging to verify retrieval mode and scope
        console.log(`üîé [CHAT_RAG] RetrievalMode=${isAdditive ? 'additive' : 'single'} | Course=${courseId} | Unit=${unitName} | LectureNames=${JSON.stringify(lectureNames)}`);

        // Retrieve top chunks from Qdrant
        const searchResults = await qdrant.searchDocuments(message, { courseId, lectureNames }, 12);

        // Log summary of results by lecture to validate scope
        try {
            const lecturesHit = Array.from(new Set((searchResults || []).map(r => r.lectureName)));
            console.log(`üìö [CHAT_RAG] Retrieved ${searchResults.length} chunks from lectures: ${lecturesHit.join(', ')}`);
            // Group by document to see which files contributed
            const byDoc = {};
            for (const r of (searchResults || [])) {
                const docId = r.documentId || 'unknown-doc';
                if (!byDoc[docId]) {
                    byDoc[docId] = {
                        fileName: r.fileName || 'unknown-filename',
                        lectures: new Set(),
                        count: 0,
                        maxScore: 0
                    };
                }
                byDoc[docId].count += 1;
                byDoc[docId].lectures.add(r.lectureName);
                if (typeof r.score === 'number' && r.score > byDoc[docId].maxScore) {
                    byDoc[docId].maxScore = r.score;
                }
            }
            const docKeys = Object.keys(byDoc);
            console.log(`üìÑ [CHAT_RAG] Documents contributing (${docKeys.length}):`);
            for (const k of docKeys) {
                const info = byDoc[k];
                const lecturesList = Array.from(info.lectures).join(', ');
                const scoreStr = info.maxScore ? info.maxScore.toFixed(3) : 'n/a';
                console.log(`   - ${info.fileName} (id=${k}) | lectures=[${lecturesList}] | chunks=${info.count} | maxScore=${scoreStr}`);
            }
        } catch (_) {}

        // Build concise context window with citations
        const citations = searchResults.map(r => ({
            lectureName: r.lectureName,
            fileName: r.fileName,
            score: r.score
        }));
        const contextText = searchResults
            .map(r => `From ${r.lectureName} (${r.fileName}):\n${r.chunkText}`)
            .join('\n\n---\n\n');
        
        // Determine source attribution based on retrieved chunks
        console.log('üîç [SOURCE_DEBUG] Retrieved chunks for source analysis:', searchResults.map(r => ({
            fileName: r.fileName,
            documentType: r.documentType,
            type: r.type,
            lectureName: r.lectureName
        })));
        
        let sourceAttribution;
        try {
            sourceAttribution = await determineSourceAttribution(searchResults, courseId, unitName, db);
            console.log('üîç [SOURCE_DEBUG] Determined source attribution:', sourceAttribution);
        } catch (error) {
            console.error('üîç [SOURCE_DEBUG] Error in source attribution:', error);
            sourceAttribution = {
                source: 'GPT',
                description: 'Generated by AI (error determining source)',
                unitName: null,
                documentType: null
            };
        }
        
        // Build the message to send to LLM
        let messageToSend = `Use only the provided course context to answer. Cite which unit a fact came from.
\n\nCourse context:\n${contextText}\n\nStudent question: ${message}`;

        // If we have conversation context (continuing a chat), use structured conversation approach
        if (conversationContext && conversationContext.conversationMessages) {
            console.log('üîÑ [CHAT_CONTINUE] Using structured conversation context');
            
            // Build the conversation history as a single message
            let conversationHistory = '';
            conversationContext.conversationMessages.forEach(msg => {
                const speaker = msg.role === 'user' ? 'Student' : 'BiocBot';
                conversationHistory += `${speaker}: ${msg.content}\n\n`;
            });
            
            // Add the student's new message
            conversationHistory += `Student: ${message}`;
            
            // Create the full message with conversation context
            messageToSend = `Use only the provided course context to answer. Cite which unit a fact came from.

Course context:
${contextText}

Previous conversation:
${conversationHistory}`;
        }

        // For now, we'll use single message approach
        // In the future, we can implement conversation persistence
        let response = await llmService.sendMessage(
            messageToSend,
            {
            // Adjust response based on student mode
            temperature: mode === 'protege' ? 0.5 : 0.5,
            maxTokens: mode === 'protege' ? 1000 : 1000,
            systemPrompt: llmService.getSystemPrompt() + 
                (mode === 'protege' ? 
                    '\n\nYou are in prot√©g√© mode. The student has demonstrated good understanding. Engage them as a study partner, ask follow-up questions, and explore topics together.' :
                    '\n\nYou are in tutor mode. The student needs guidance. Provide clear explanations, examples, and step-by-step help.'
                )
        });
        
        let fullContent = response && response.content ? response.content : '';

        // Detect truncation and auto-continue up to N times
        function extractFinishReason(resp) {
            try {
                return (resp && (resp.finishReason || resp.finish_reason || (resp.usage && resp.usage.finish_reason) || resp.stopReason || resp.stop_reason)) || '';
            } catch (e) { return ''; }
        }
        function isLikelyTruncated(resp, content) {
            const fr = (extractFinishReason(resp) + '').toLowerCase();
            if (fr.includes('length') || fr.includes('token')) return true;
            if (!content) return false;
            const tail = content.slice(-60);
            const endsClean = /[\.\!\?]\s*$/.test(tail);
            return !endsClean && content.length > 300;
        }
        
        const MAX_CONTINUATIONS = 2;
        let cont = 0;
        while (cont < MAX_CONTINUATIONS && isLikelyTruncated(response, fullContent)) {
            cont += 1;
            console.log(`‚è© [CHAT_CONTINUE] Requesting continuation ${cont}; current length=${fullContent.length}`);
            const tailSnippet = fullContent.slice(-200);
            const contPrompt = `Continue the previous answer. Do not repeat earlier content. Pick up seamlessly from here: "${tailSnippet}"`;
            const contResp = await llmService.sendMessage(contPrompt, {
                temperature: mode === 'protege' ? 0.8 : 0.6,
                maxTokens: mode === 'protege' ? 800 : 800,
                systemPrompt: llmService.getSystemPrompt() + 
                    (mode === 'protege' ? 
                        '\n\nYou are in prot√©g√© mode. The student has demonstrated good understanding. Continue the explanation succinctly.' :
                        '\n\nYou are in tutor mode. Continue the explanation clearly and step-by-step.'
                    )
            });
            const chunk = contResp && contResp.content ? contResp.content : '';
            console.log(`üìé [CHAT_CONTINUE] Received chunk ${cont} length=${chunk.length}`);
            if (chunk) {
                fullContent += (fullContent.endsWith('\n') ? '' : '\n') + chunk;
            }
            response = contResp;
        }
        
        // Format response for frontend
        const chatResponse = {
            success: true,
            message: fullContent,
            model: response.model,
            usage: response.usage,
            timestamp: new Date().toISOString(),
            mode: mode || 'default',
            citations,
            sourceAttribution,
            debug: {
                searchResultsCount: searchResults.length,
                avgScore: searchResults.length > 0 ? searchResults.reduce((sum, result) => sum + (result.score || 0), 0) / searchResults.length : 0,
                maxScore: searchResults.length > 0 ? Math.max(...searchResults.map(result => result.score || 0)) : 0,
                documentTypes: searchResults.map(r => ({ fileName: r.fileName, documentType: r.documentType, type: r.type, score: r.score }))
            },
            retrieval: {
                mode: isAdditive ? 'additive' : 'single',
                lectureNames
            }
        };
        
        console.log(`‚úÖ Chat response sent successfully`);
        
        res.json(chatResponse);
        
    } catch (error) {
        console.error('‚ùå Error in chat endpoint:', error);
        
        // Provide user-friendly error messages
        let errorMessage = 'Sorry, I encountered an error processing your message.';
        let statusCode = 500;
        
        if (error.message.includes('OLLAMA_ENDPOINT')) {
            errorMessage = 'Ollama service is not available. Please check if Ollama is running.';
            statusCode = 503;
        } else if (error.message.includes('API key')) {
            errorMessage = 'Authentication error. Please check your API configuration.';
            statusCode = 401;
        } else if (error.message.includes('endpoint')) {
            errorMessage = 'Service endpoint is not reachable. Please check your configuration.';
            statusCode = 503;
        }
        
        res.status(statusCode).json({
            success: false,
            message: errorMessage,
            error: process.env.NODE_ENV === 'development' ? error.message : undefined,
            timestamp: new Date().toISOString()
        });
    }
});


/**
 * GET /api/chat/status
 * Get the current status of the LLM service
 */
router.get('/status', async (req, res) => {
    try {
        const llmService = req.app.locals.llm;
        
        if (!llmService) {
            return res.status(503).json({
                success: false,
                message: 'LLM service is not initialized'
            });
        }
        
        const status = llmService.getStatus();
        
        res.json({
            success: true,
            data: status
        });
        
    } catch (error) {
        console.error('‚ùå Error getting chat status:', error);
        
        res.status(500).json({
            success: false,
            message: 'Failed to get chat status',
            error: process.env.NODE_ENV === 'development' ? error.message : undefined
        });
    }
});



/**
 * POST /api/chat/test
 * Test the LLM connection
 */
router.post('/test', async (req, res) => {
    try {
        console.log('üß™ Testing LLM connection...');
        
        const llmService = req.app.locals.llm;
        
        if (!llmService) {
            return res.status(503).json({
                success: false,
                message: 'LLM service is not initialized'
            });
        }
        
        const isConnected = await llmService.testConnection();
        
        if (isConnected) {
            res.json({
                success: true,
                message: 'LLM connection test successful',
                provider: llmService.getProviderName(),
                timestamp: new Date().toISOString()
            });
        } else {
            res.status(503).json({
                success: false,
                message: 'LLM connection test failed',
                provider: llmService.getProviderName(),
                timestamp: new Date().toISOString()
            });
        }
        
    } catch (error) {
        console.error('‚ùå Error testing LLM connection:', error);
        
        res.status(500).json({
            success: false,
            message: 'Failed to test LLM connection',
            error: process.env.NODE_ENV === 'development' ? error.message : undefined,
            timestamp: new Date().toISOString()
        });
    }
});

/**
 * GET /api/chat/models
 * Get available models from the current provider
 */
router.get('/models', async (req, res) => {
    try {
        const llmService = req.app.locals.llm;
        
        if (!llmService) {
            return res.status(503).json({
                success: false,
                message: 'LLM service is not initialized'
            });
        }
        
        const models = await llmService.getAvailableModels();
        
        res.json({
            success: true,
            data: {
                models: models,
                provider: llmService.getProviderName(),
                timestamp: new Date().toISOString()
            }
        });
        
    } catch (error) {
        console.error('‚ùå Error getting available models:', error);
        
        res.status(500).json({
            success: false,
            message: 'Failed to get available models',
            error: process.env.NODE_ENV === 'development' ? error.message : undefined
        });
    }
});

/**
 * POST /api/chat/save
 * Save a chat session to the database for instructor access
 */
router.post('/save', async (req, res) => {
    try {
        const { 
            sessionId, 
            courseId, 
            studentId, 
            studentName, 
            unitName, 
            title, 
            messageCount, 
            duration, 
            savedAt, 
            chatData 
        } = req.body;
        
        // Validate required fields
        if (!sessionId || !courseId || !studentId || !studentName) {
            return res.status(400).json({
                success: false,
                message: 'Missing required fields: sessionId, courseId, studentId, studentName'
            });
        }
        
        // Get database instance from app.locals
        const db = req.app.locals.db;
        if (!db) {
            return res.status(503).json({
                success: false,
                message: 'Database connection not available'
            });
        }
        
        // Save chat session to database
        const chatSessionsCollection = db.collection('chat_sessions');
        
        const sessionData = {
            sessionId,
            courseId,
            studentId,
            studentName,
            unitName: unitName || 'Unknown Unit',
            title: title || `Chat Session ${new Date().toLocaleDateString()}`,
            messageCount: messageCount || 0,
            duration: duration || 'Unknown',
            savedAt: savedAt || new Date().toISOString(),
            chatData: chatData || {},
            isDeleted: false, // Soft deletion flag
            createdAt: new Date()
        };
        
        // Insert or update the session
        await chatSessionsCollection.replaceOne(
            { sessionId: sessionId },
            sessionData,
            { upsert: true }
        );
        
        console.log(`Chat session saved: ${sessionId} for student ${studentName} in course ${courseId}`);
        
        res.json({
            success: true,
            message: 'Chat session saved successfully',
            data: {
                sessionId: sessionId,
                courseId: courseId,
                studentId: studentId
            }
        });
        
    } catch (error) {
        console.error('Error saving chat session:', error);
        res.status(500).json({
            success: false,
            message: 'Internal server error while saving chat session'
        });
    }
});

module.exports = router; 